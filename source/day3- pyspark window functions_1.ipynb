{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6bff1f7-1ec2-45c7-85dd-33011b0d49b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+---------+------------+\n|order_id|order_date| customer_name|     city|order_amount|\n+--------+----------+--------------+---------+------------+\n|    1001|2017-04-01|   David Smith|GuildFord|     10000.0|\n|    1002|2017-04-02|   David Jones|Arlington|     20000.0|\n|    1003|2017-04-03|    John Smith| Shalford|      5000.0|\n|    1004|2017-04-04| Michael Smith|GuildFord|     15000.0|\n|    1005|2017-04-05|David Williams| Shalford|      7000.0|\n+--------+----------+--------------+---------+------------+\nonly showing top 5 rows\n\n+------------------+----------------+----------------+----------------+-----------+\n|total_order_amount|max_order_amount|min_order_amount|avg_order_amount|order_count|\n+------------------+----------------+----------------+----------------+-----------+\n|          100500.0|         25000.0|           500.0|         10050.0|         10|\n+------------------+----------------+----------------+----------------+-----------+\n\n+--------+----------+--------------+---------+------------+----+----------+----------+-----+----------------+-----------------+------------------------+-----------------------+\n|order_id|order_date| customer_name|     city|order_amount|rank|dense_rank|row_number|ntile|lag_order_amount|lead_order_amount|first_value_order_amount|last_value_order_amount|\n+--------+----------+--------------+---------+------------+----+----------+----------+-----+----------------+-----------------+------------------------+-----------------------+\n|    1010|2017-04-25|   Peter Smith|GuildFord|       500.0|   1|         1|         1|    1|             0.0|           1000.0|                   500.0|                  500.0|\n|    1009|2017-04-20|  Robert Smith| Shalford|      1000.0|   2|         2|         2|    1|           500.0|           2000.0|                   500.0|                 1000.0|\n|    1008|2017-04-11|   David Brown|Arlington|      2000.0|   3|         3|         3|    1|          1000.0|           5000.0|                   500.0|                 2000.0|\n|    1003|2017-04-03|    John Smith| Shalford|      5000.0|   4|         4|         4|    1|          2000.0|           7000.0|                   500.0|                 5000.0|\n|    1005|2017-04-05|David Williams| Shalford|      7000.0|   5|         5|         5|    2|          5000.0|          10000.0|                   500.0|                 7000.0|\n+--------+----------+--------------+---------+------------+----+----------+----------+-----+----------------+-----------------+------------------------+-----------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,lit,sum,max,min,avg,count,rank,dense_rank,row_number,ntile,lag,lead,first,last\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"AggregateWindowFunctions\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1001, '2017-04-01', 'David Smith', 'GuildFord', 10000.00),\n",
    "    (1002, '2017-04-02', 'David Jones', 'Arlington', 20000.00),\n",
    "    (1003, '2017-04-03', 'John Smith', 'Shalford', 5000.00),\n",
    "    (1004, '2017-04-04', 'Michael Smith', 'GuildFord', 15000.00),\n",
    "    (1005, '2017-04-05', 'David Williams', 'Shalford', 7000.00),\n",
    "    (1006, '2017-04-06', 'Paum Smith', 'GuildFord', 25000.00),\n",
    "    (1007, '2017-04-10', 'Andrew Smith', 'Arlington', 15000.00),\n",
    "    (1008, '2017-04-11', 'David Brown', 'Arlington', 2000.00),\n",
    "    (1009, '2017-04-20', 'Robert Smith', 'Shalford', 1000.00),\n",
    "    (1010, '2017-04-25', 'Peter Smith', 'GuildFord', 500.00)\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"order_id\", \"order_date\", \"customer_name\", \"city\", \"order_amount\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show(5)\n",
    "\n",
    "# Aggregation functions\n",
    "df.agg(\n",
    "    sum(\"order_amount\").alias(\"total_order_amount\"),\n",
    "    max(\"order_amount\").alias(\"max_order_amount\"),\n",
    "    min(\"order_amount\").alias(\"min_order_amount\"),\n",
    "    avg(\"order_amount\").alias(\"avg_order_amount\"),\n",
    "    count(\"*\").alias(\"order_count\")\n",
    ").show(5)\n",
    "\n",
    "# Window functions\n",
    "window_spec = Window.orderBy(\"order_amount\")\n",
    "\n",
    "df.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "  .withColumn(\"dense_rank\", dense_rank().over(window_spec)) \\\n",
    "  .withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "  .withColumn(\"ntile\", ntile(3).over(window_spec)) \\\n",
    "  .withColumn(\"lag_order_amount\", lag(\"order_amount\").over(window_spec)) \\\n",
    "  .withColumn(\"lead_order_amount\", lead(\"order_amount\").over(window_spec)) \\\n",
    "  .withColumn(\"first_value_order_amount\", first(\"order_amount\").over(window_spec)) \\\n",
    "  .withColumn(\"last_value_order_amount\", last(\"order_amount\").over(window_spec)).fillna(0) \\\n",
    "  .show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d29ab7-077e-4379-87de-551d048f4887",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+----------+------------------+-----------+\n|     city|total_amount|max_amount|min_amount|        avg_amount|order_count|\n+---------+------------+----------+----------+------------------+-----------+\n|GuildFord|     50500.0|   25000.0|     500.0|           12625.0|          4|\n|Arlington|     37000.0|   20000.0|    2000.0|12333.333333333334|          3|\n| Shalford|     13000.0|    7000.0|    1000.0| 4333.333333333333|          3|\n+---------+------------+----------+----------+------------------+-----------+\n\n+--------+----------+--------------+---------+------------+----+----------+----------+-----+----------+-----------+------------+-----------+\n|order_id|order_date| customer_name|     city|order_amount|rank|dense_rank|row_number|ntile|lag_amount|lead_amount|first_amount|last_amount|\n+--------+----------+--------------+---------+------------+----+----------+----------+-----+----------+-----------+------------+-----------+\n|    1008|2017-04-11|   David Brown|Arlington|      2000.0|   1|         1|         1|    1|      null|    15000.0|      2000.0|     2000.0|\n|    1007|2017-04-10|  Andrew Smith|Arlington|     15000.0|   2|         2|         2|    2|    2000.0|    20000.0|      2000.0|    15000.0|\n|    1002|2017-04-02|   David Jones|Arlington|     20000.0|   3|         3|         3|    3|   15000.0|       null|      2000.0|    20000.0|\n|    1010|2017-04-25|   Peter Smith|GuildFord|       500.0|   1|         1|         1|    1|      null|    10000.0|       500.0|      500.0|\n|    1001|2017-04-01|   David Smith|GuildFord|     10000.0|   2|         2|         2|    1|     500.0|    15000.0|       500.0|    10000.0|\n|    1004|2017-04-04| Michael Smith|GuildFord|     15000.0|   3|         3|         3|    2|   10000.0|    25000.0|       500.0|    15000.0|\n|    1006|2017-04-06|    Paum Smith|GuildFord|     25000.0|   4|         4|         4|    3|   15000.0|       null|       500.0|    25000.0|\n|    1009|2017-04-20|  Robert Smith| Shalford|      1000.0|   1|         1|         1|    1|      null|     5000.0|      1000.0|     1000.0|\n|    1003|2017-04-03|    John Smith| Shalford|      5000.0|   2|         2|         2|    2|    1000.0|     7000.0|      1000.0|     5000.0|\n|    1005|2017-04-05|David Williams| Shalford|      7000.0|   3|         3|         3|    3|    5000.0|       null|      1000.0|     7000.0|\n+--------+----------+--------------+---------+------------+----+----------+----------+-----+----------+-----------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, sum, max, min, avg, count, rank, dense_rank, row_number, ntile, lag, lead, first, last\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OrderAnalysis\").getOrCreate()\n",
    "\n",
    "# Assuming you have a DataFrame named orders_df with the provided data\n",
    "data = [\n",
    "    (1001, '2017-04-01', 'David Smith', 'GuildFord', 10000.00),\n",
    "    (1002, '2017-04-02', 'David Jones', 'Arlington', 20000.00),\n",
    "    (1003, '2017-04-03', 'John Smith', 'Shalford', 5000.00),\n",
    "    (1004, '2017-04-04', 'Michael Smith', 'GuildFord', 15000.00),\n",
    "    (1005, '2017-04-05', 'David Williams', 'Shalford', 7000.00),\n",
    "    (1006, '2017-04-06', 'Paum Smith', 'GuildFord', 25000.00),\n",
    "    (1007, '2017-04-10', 'Andrew Smith', 'Arlington', 15000.00),\n",
    "    (1008, '2017-04-11', 'David Brown', 'Arlington', 2000.00),\n",
    "    (1009, '2017-04-20', 'Robert Smith', 'Shalford', 1000.00),\n",
    "    (1010, '2017-04-25', 'Peter Smith', 'GuildFord', 500.00)\n",
    "]\n",
    "\n",
    "schema = [\"order_id\", \"order_date\", \"customer_name\", \"city\", \"order_amount\"]\n",
    "orders_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Define a window specification based on the 'city' column\n",
    "window_spec = Window.partitionBy(\"city\").orderBy(\"order_amount\")\n",
    "\n",
    "# Aggregate functions\n",
    "agg_df = orders_df.groupBy(\"city\").agg(\n",
    "    sum(\"order_amount\").alias(\"total_amount\"),\n",
    "    max(\"order_amount\").alias(\"max_amount\"),\n",
    "    min(\"order_amount\").alias(\"min_amount\"),\n",
    "    avg(\"order_amount\").alias(\"avg_amount\"),\n",
    "    count(\"order_amount\").alias(\"order_count\")\n",
    ")\n",
    "\n",
    "# Window functions\n",
    "window_df = orders_df.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(window_spec)) \\\n",
    "    .withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"ntile\", ntile(3).over(window_spec)) \\\n",
    "    .withColumn(\"lag_amount\", lag(\"order_amount\").over(window_spec)) \\\n",
    "    .withColumn(\"lead_amount\", lead(\"order_amount\").over(window_spec)) \\\n",
    "    .withColumn(\"first_amount\", first(\"order_amount\").over(window_spec)) \\\n",
    "    .withColumn(\"last_amount\", last(\"order_amount\").over(window_spec))\n",
    "\n",
    "# Show the results\n",
    "agg_df.show()\n",
    "window_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "day3- pyspark window functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
